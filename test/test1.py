# -*- coding: utf-8 -*-
"""atrf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hmB_LrM0XxUu3DWZ8rORzDr7k27QbKTP
"""

# https://www.fool.com/earnings/call-transcripts/2020/04/29/alphabet-inc-goog-googl-q1-2020-earnings-call-tran.aspx
# https://www.fool.com/earnings/call-transcripts/2020/07/31/alphabet-inc-goog-googl-q2-2020-earnings-call-tran.aspx
# https://www.fool.com/earnings/call-transcripts/2019/10/29/google-inc-googl-q3-2019-earnings-call-transcript.aspx
# https://www.fool.com/earnings/call-transcripts/2020/02/03/alphabet-inc-goog-googl-q4-2019-earnings-call-tran.aspx

#https://www.fool.com/earnings/call-transcripts/2021/05/18/wal-mart-stores-inc-wmt-q1-2022-earnings-call-tran/
#https://www.fool.com/earnings/call-transcripts/2021/08/17/wal-mart-inc-wmt-q2-2022-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2021/11/16/walmart-inc-wmt-q3-2022-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2022/02/17/walmart-inc-wmt-q4-2022-earnings-call-transcript/

#https://www.fool.com/earnings/call-transcripts/2021/04/21/netflix-nflx-q1-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2021/07/20/netflix-nflx-q2-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2021/10/20/netflix-nflx-q3-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2022/01/21/netflix-nflx-q4-2021-earnings-call-transcript/

#https://www.fool.com/earnings/call-transcripts/2021/04/27/tesla-tsla-q1-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2021/07/27/tesla-tsla-q2-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2021/10/21/tesla-tsla-q3-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2022/01/27/tesla-tsla-q4-2021-earnings-call-transcript/

#https://www.fool.com/earnings/call-transcripts/2021/04/27/advanced-micro-devices-amd-q1-2021-earnings-call-t/
#https://www.fool.com/earnings/call-transcripts/2021/07/27/advanced-micro-devices-amd-q2-2021-earnings-call-t/
#https://www.fool.com/earnings/call-transcripts/2021/10/27/advanced-micro-devices-amd-q3-2021-earnings-call-t/
#https://www.fool.com/earnings/call-transcripts/2022/02/02/advanced-micro-devices-amd-q4-2021-earnings-call-t/

#https://www.fool.com/earnings/call-transcripts/2021/04/22/intel-intc-q1-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2021/07/23/intel-intc-q2-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2021/10/22/intel-intc-q3-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2022/01/27/intel-intc-q4-2021-earnings-call-transcript/

#https://www.fool.com/earnings/call-transcripts/2020/08/20/alibaba-group-holding-ltd-baba-q1-2021-earnings-ca/
#https://www.fool.com/earnings/call-transcripts/2020/11/05/alibaba-group-holding-ltd-baba-q2-2021-earnings-ca/
#https://www.fool.com/earnings/call-transcripts/2021/02/02/alibaba-group-holding-ltd-baba-q3-2021-earnings-ca/
#https://www.fool.com/earnings/call-transcripts/2021/05/13/alibaba-group-holding-ltd-baba-q4-2021-earnings-ca/

#https://www.fool.com/earnings/call-transcripts/2021/04/29/ford-f-q1-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2021/07/28/ford-f-q2-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2021/10/28/ford-f-q3-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2022/02/04/ford-f-q4-2021-earnings-call-transcript/

#https://www.fool.com/earnings/call-transcripts/2021/04/29/twitter-inc-twtr-q1-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2021/07/22/twitter-inc-twtr-q2-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2021/10/27/twitter-inc-twtr-q3-2021-earnings-call-transcript/
#https://www.fool.com/earnings/call-transcripts/2022/02/10/twitter-twtr-q4-2021-earnings-call-transcript/


# https://www.fool.com/earnings/call-transcripts/2021/09/17/oracle-orcl-q1-2022-earnings-call-transcript/
# https://www.fool.com/earnings/call-transcripts/2021/12/10/oracle-orcl-q2-2022-earnings-call-transcript/
# https://www.fool.com/earnings/call-transcripts/2022/03/11/oracle-orcl-q3-2022-earnings-call-transcript/
# https://www.fool.com/earnings/call-transcripts/2022/06/13/oracle-orcl-q4-2022-earnings-call-transcript/

# https://www.fool.com/earnings/call-transcripts/2020/11/12/cisco-systems-csco-q1-2021-earnings-call-transcrip/
# https://www.fool.com/earnings/call-transcripts/2021/02/10/cisco-systems-csco-q2-2021-earnings-call-transcrip/
# https://www.fool.com/earnings/call-transcripts/2021/05/19/cisco-systems-csco-q3-2021-earnings-call-transcrip/
# https://www.fool.com/earnings/call-transcripts/2021/08/19/cisco-systems-csco-q4-2021-earnings-call-transcrip/

# https://www.fool.com/earnings/call-transcripts/2020/04/21/sap-se-on-sap-q1-2020-earnings-call-transcript.aspx
# https://www.fool.com/earnings/call-transcripts/2020/07/27/sap-se-on-sap-q2-2020-earnings-call-transcript.aspx
# https://www.fool.com/earnings/call-transcripts/2020/10/26/sap-se-on-sap-q3-2020-earnings-call-transcript/
# https://www.fool.com/earnings/call-transcripts/2021/01/29/sap-se-on-sap-q4-2020-earnings-call-transcript/

# /content/Mindtree_Transcript_of_analyst_call _Q1FY22.pdf
# /content/Mindtree_Transcript of analyst call_ Q2FY22_0.pdf
# /content/Mindtree_Transcript_Earnings_Call_Q3FY22.pdf
# /content/Transcript Earnings Call Q4 FY22_0.pdf



# Commented out IPython magic to ensure Python compatibility.

from bs4 import BeautifulSoup
from datetime import datetime
import requests
import pandas as pd
import numpy as np
import re
from pathlib import Path
import spacy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
import gensim
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from pandas_datareader import data as pdr
from datetime import timedelta
import yfinance as yfin
yfin.pdr_override()
# %matplotlib inline


spacy_model = 'en_core_web_sm'
nlp = spacy.load(spacy_model)


from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfpage import PDFPage
from io import StringIO
import docx2txt
from striprtf.striprtf import rtf_to_text




choice = int(input("1.Web  2.Documents : "))
if(choice == 1):
  columns = ['ticker','quarter', 'year', 'url']

  data = [['TTM','q1', 2022, 'https://www.fool.com/earnings/call-transcripts/2021/07/27/tata-motors-ttm-q1-2022-earnings-call-transcript/'],
          ['TTM','q2', 2022, 'https://www.fool.com/earnings/call-transcripts/2021/11/02/tata-motors-ttm-q2-2022-earnings-call-transcript/'],
          ['TTM','q3', 2021, 'https://www.fool.com/earnings/call-transcripts/2021/01/31/tata-motors-ttm-q3-2021-earnings-call-transcript/'],
          ['TTM','q4', 2021, 'https://www.fool.com/earnings/call-transcripts/2021/05/18/tata-motors-ttm-q4-2021-earnings-call-transcript/'],
 ['GOOGL','q1', 2020, 'https://www.fool.com/earnings/call-transcripts/2020/04/29/alphabet-inc-goog-googl-q1-2020-earnings-call-tran.aspx'],
          ['GOOGL','q2', 2020, 'https://www.fool.com/earnings/call-transcripts/2020/07/31/alphabet-inc-goog-googl-q2-2020-earnings-call-tran.aspx'],
          ['GOOGL','q3', 2020, 'https://www.fool.com/earnings/call-transcripts/2019/10/29/google-inc-googl-q3-2019-earnings-call-transcript.aspx'],
          ['GOOGL','q4', 2020, 'https://www.fool.com/earnings/call-transcripts/2020/02/03/alphabet-inc-goog-googl-q4-2019-earnings-call-tran.aspx']
         
          ]

  #TAKING INPUT FROM USER
  # number_of_inps = int(input("Enter the no. inputs"))       
  # data = []
  # if(number_of_inps%4 == 0):
  #   for i in range(number_of_inps):
  #     input_data = [input("Ticker: "), input("quarter: "), input("year: "), input("url: ")]
  #     data.append(input_data)

  df = pd.DataFrame(data=data, columns=columns)
  df['call_date'] = df['url'].apply(lambda x : pd.to_datetime(re.match(r".*(\d{4}/\d{2}/\d{2})", x).group(1)))

  def get_text(df_row):
    print(f"Processing {df_row['url']}")
    page = requests.get(df_row['url'])
    regex = re.compile(r'Prepared Remarks:\s*(\n*.*)\s*Duration:',re.DOTALL)
    matches = regex.finditer(page.text)
    plain_text=''
    for match in matches:
        # print(match)
        plain_text = BeautifulSoup(match.group(1), 'html.parser').get_text(separator='') # remove all the html
        plain_text = re.sub("\n|\r", ".", plain_text, flags=re.MULTILINE) # remove new line
        plain_text = re.sub("\s\s+|\t", " ", plain_text)
    return plain_text
    
  df['plain_text'] = df.apply(get_text, axis=1)
elif(choice == 2):
  columns = ['ticker','quarter', 'year', 'url', 'call_date']
  number_of_inps = 8
  # number_of_inps = int(input("Enter the no. inputs"))       
  # data = []
  # if(number_of_inps%4 == 0):
  #   for i in range(number_of_inps):
  #     input_data = [input("Ticker: "), input("quarter: "), input("year: "), input("url: "), input("call_date: ")]
  #     data.append(input_data)

  data = [
          ['TATAPOWER','q1', 2020, 'analyst-call-transcript-q1-fy20.pdf','2019-08-01'],
          ['TATAPOWER','q2', 2020, 'analyst-call-transcript-q2-fy20.pdf','2019-11-08'],
      ['TATAPOWER','q3', 2020, 'analyst-call-transcript-q4-fy20.pdf','2020-01-29'],
      ['TATAPOWER','q4', 2020, 'analyst-call-transcript-q4-fy20.pdf','2020-03-19'],
      
      
      ['TATACHEM','q1', 2022, 'hknfbhbZ9DcVZJB117.docx','2021-08-06'],
          ['TATACHEM','q2', 2022, 'tcl-q2-fy22-earnings-call-transcript.docx','2021-10-28'],
      ['TATACHEM','q3', 2022, 'tcl-q3-fy22-earnings-call-transcript.docx','2022-02-11'],
      ['TATACHEM','q4', 2022, '/content/tcl-q4-fy22-earnings-call-transcript.docx','2022-04-30']

      ]
  
  df = pd.DataFrame(data=data, columns=columns)
  df['call_date'] = pd.to_datetime(df['call_date'])
  # print(df['call_date'])

  def convert_rtf_to_text(path):
      with open(path) as file:
          content = file.read()
      text = rtf_to_text(content)
      return text

  def convert_docx_to_txt(path):
      text = docx2txt.process(path)
      return text

  def convert_pdf_to_txt(path):
    rsrcmgr = PDFResourceManager()
    retstr = StringIO()
    # codec = 'utf-8'
    laparams = LAParams()
    device = TextConverter(rsrcmgr, retstr, laparams=laparams)
    fp = open(path, 'rb')
    interpreter = PDFPageInterpreter(rsrcmgr, device)
    maxpages = 0
    caching = True
    pagenos=set()

    for page in PDFPage.get_pages(fp, pagenos, caching=caching, check_extractable=True):
        interpreter.process_page(page)
    text = retstr.getvalue()
    fp.close()
    device.close()
    retstr.close()
    return text

  # filepath='C:/Users/userpc/Documents/file-sample_100kB.rtf'
  text=""
  # iter_txt = []
  for i in range(number_of_inps):
    file_extns = df['url'][i].split(".")
    if(file_extns[-1]=="pdf"):
        print(f"Processing {df['url'][i]}")
        text = convert_pdf_to_txt(df['url'][i])
        text = re.sub("\n|\r", ".", text, flags=re.MULTILINE)
        text = re.sub("\s\s+|\t", " ", text)
        # print(text)
        df.at[i,'plain_text'] = text
        # print(df['plain_text'])
    elif(file_extns[-1]=="rtf"):
        print(f"Processing {df['url'][i]}")
        text = convert_rtf_to_text(df['url'][i])
        text = re.sub("\n|\r", ".", text, flags=re.MULTILINE)
        text = re.sub("\s\s+|\t", " ", text)
        df.at[i,'plain_text'] = text
    else:
        print(f"Processing {df['url'][i]}")
        text = convert_docx_to_txt(df['url'][i])
        text = re.sub("\n|\r", ".", text, flags=re.MULTILINE)
        text = re.sub("\s\s+|\t", " ", text)
        df.at[i,'plain_text'] = text

# print(df)



additional_stop_words = ['hi', 'earning', 'conference', 'speaker', 'analyst', 'operator', 'welcome', \
                         'think', 'cost', 'result', 'primarily', 'overall', 'line', 'general', \
                          'thank', 'see', 'alphabet', 'google', 'facebook', 'amazon', 'microsoft',\
                        'business', 'customer', 'revenue', 'question', 'lady', 'gentleman', \
                        'continue', 'continuing', 'continued', 'focus', 'participant', 'see', 'seeing', \
                        'user', 'work', 'lot', 'day',  'like', 'looking', 'look', 'come', 'yes', 'include', \
                        'investor', 'director', 'expense', 'manager', 'founder', 'chairman', \
                         'chief', 'operating', 'officer', 'executive', 'financial', 'senior', 'vice', 'president', \
                        'opportunity', 'go', 'expect', 'increase', 'quarter', 'stand', 'instructions', \
                        'obviously', 'thing', 'important', 'help', 'bring', 'mention', 'yeah', 'get', 'proceed', \
                        'currency', 'example', 'believe', '.','Â©'] 

for stopword in additional_stop_words:
    nlp.vocab[stopword].is_stop = True

def lemma_text(df_row):
  print(f"Processing {df_row['ticker']}, quarter: {df_row['quarter']}, year: {df_row['year']}")
  clean_text = []

  doc = nlp(df_row['plain_text'])
  with doc.retokenize() as retokenizer:
      for ent in doc.ents:
          # print(ent.text, ent.label_)
          retokenizer.merge(doc[ent.start:ent.end], attrs={"LEMMA": ent.text})
  #print('-------------------')
  for word in doc:
      # print(word, word.lemma_, word.ent_type_)
      if word.is_alpha and word.is_ascii and not word.is_stop and \
          word.ent_type_ not in ['PERSON','DATE', 'TIME', 'ORDINAL', 'CARDINAL'] and \
          word.text.lower() not in additional_stop_words and \
          word.lemma_.lower() not in additional_stop_words:
              # print(word)
              clean_text.append(word.lemma_.lower())
  print(clean_text)
  return clean_text

df['clean_text'] = df.apply(lemma_text, axis=1)
print(df['clean_text'])

tickers = df['ticker'].unique().tolist()
for ticker in tickers:
    bigram = gensim.models.Phrases(df.loc[df['ticker']==ticker,'clean_text'], min_count=3)
    df.loc[df['ticker']==ticker,'words_ngrams'] = df.loc[df['ticker']==ticker,'clean_text'].apply(lambda x : bigram[x])

additional_stop_words=['gross_margin', 'come_line', 'constant_currency', 'operatorthank_proceed', 'long_term', \
                      'grow_constant', 'year_year' , 'growth', 'strong', 'great', 'maybe','f_moderator','good_organize','managing_ceo', 'operatorhello_amd']

def remove_bigrams(df_row):
  print(f"Processing {df_row['ticker']}, quarter: {df_row['quarter']}, year: {df_row['year']}")
  words = []
  doc = df_row['words_ngrams']
  for word in doc:
      if word not in additional_stop_words:
          words.append(word)
  return words

df['words_ngrams'] = df.apply(remove_bigrams, axis=1)
print(df['words_ngrams'])

tfidf_vectorizer = TfidfVectorizer(smooth_idf=True,use_idf=True, min_df=0.025)

def get_wordcloud():
  tickers = df['ticker'].unique().tolist()
  num_top_words=100
  wc = WordCloud(width = 1000, height = 1000, 
                background_color ='black', 
                min_font_size = 20,
                max_font_size=150,
                colormap='Blues_r')
                # colormap='Wistia')
  docs_joined = []
  num_tickers = len(tickers)
  for ticker in tickers:
      
      sub_df = df.loc[df['ticker']==ticker]
      sub_df = sub_df.sort_values('call_date',ascending=True)
      docs = sub_df['words_ngrams'].apply(lambda x: " ".join(x)).tolist()
      dates = sub_df['call_date'].tolist()
      # print(dates)
      num_dates = len(dates)

      idf_vector = tfidf_vectorizer.fit_transform(docs)
      index=0
      fig = plt.figure(figsize=(15,15))
      for dt, doc in zip(dates, docs):
          docs_joined.extend(doc)
          
          df_idf = pd.DataFrame(idf_vector[index].T.toarray(), index=tfidf_vectorizer.get_feature_names_out(),columns=["idf_weights"])
          dict_idf_topwrds = df_idf.sort_values(by=['idf_weights'],ascending=False)[:num_top_words].to_dict()['idf_weights']

          # if(choice == 'web'):
          plt.subplot(1*num_tickers, num_dates,index+1).set_title("WordCloud-" + ticker + " : " + str(dt.date()))
          # else:
          #   plt.subplot(1*num_tickers, num_dates,index+1).set_title("WordCloud-" + ticker + " : " + df['call_date'][index])
          wc.generate_from_frequencies(dict_idf_topwrds)
          # plot the WordCloud image                        
          plt.imshow(wc)
          plt.axis("off") 
          plt.tight_layout(pad = 1) 
          index += 1
  plt.show()

get_wordcloud()

sentiments = ['negative', 'positive', 'uncertainty', 'litigious', 'constraining', 'interesting']

sentiment_df = pd.read_csv('https://raw.githubusercontent.com/ark4innovation/datascience/master/ai-for-trading/5-nlp-on-financial-statements/loughran_mcdonald_master_dic_2016.csv')
sentiment_df.columns = [column.lower() for column in sentiment_df.columns]

# Remove unused information
sentiment_df = sentiment_df[sentiments + ['word']]
sentiment_df[sentiments] = sentiment_df[sentiments].astype(bool)
print(f'before any sentiment_df.shape {sentiment_df.shape}')
sentiment_df = sentiment_df[(sentiment_df[sentiments]).any(1)]
print(f'after any sentiment_df.shape {sentiment_df.shape}')

word_list = nlp(" ".join(sentiment_df['word'].str.lower()))
word_lemmas = []
for word in word_list:
    # print(word.text, word.lemma_)
    word_lemmas.append(word.lemma_)

sentiment_df.insert(loc=7, column='lemma', value=word_lemmas)

sentiment_df = sentiment_df.drop_duplicates('lemma')
print(f'after drop_duplicates sentiment_df.shape {sentiment_df.shape}')
print()
print(f'sentiment_df[sentiments].sum()-->\n{sentiment_df[sentiments].sum()}')

df = df.reindex(columns= df.columns.to_list() + sentiments)

sentiment_vectorizer = CountVectorizer()

def get_sentiment_info(df_row, sentiment_vectorizer):
    vector = sentiment_vectorizer.transform([" ".join(df_row['clean_text'])])
    return np.sum(vector.toarray())

for sentiment in sentiments:
    sentiment_words = sentiment_df.loc[sentiment_df[sentiment],'lemma']
    sentiment_vectorizer.fit(sentiment_words)
    df[sentiment] = df.apply(get_sentiment_info, args=(sentiment_vectorizer, ), axis=1)

from matplotlib.transforms import Bbox
fig = plt.figure(figsize=(15, 15))
tickers = df['ticker'].unique().tolist()
for index, ticker in enumerate(tickers):
    pos = 220 + index + 1 # to create fig.add_subplot(221), fig.add_subplot(222), fig.add_subplot(223), fig.add_subplot(224)
    ax1 = fig.add_subplot(pos)
    sub_df = df.loc[df['ticker']==ticker,['call_date']+sentiments]
    sub_df = sub_df.sort_values('call_date',ascending=True)
    ax1.plot(sub_df['call_date'], sub_df[sentiments], marker='s')
    plt.xlabel('Date')
    plt.ylabel('No. of words')
    ax1.set_title(ticker)
    plt.setp(ax1.get_xticklabels(), rotation=30, horizontalalignment='right')
    ax1.legend(sentiments, loc='upper left')
plt.savefig('testimage.png', bbox_inches = 'tight')
plt.show()

days_after=30
tickers = df['ticker'].unique().tolist()

start_date = df['call_date'].min()
end_date = df['call_date'].max() + timedelta(days=days_after)

print(start_date, end_date)
ohlc_df =  pdr.get_data_yahoo(tickers, start_date, end_date)
close_df = ohlc_df['Close']

fig = plt.figure(figsize=(15, 15))
fig.autofmt_xdate()
tickers = df['ticker'].unique().tolist()
for index, ticker in enumerate(tickers):
    pos = 220 + index + 1 # to create fig.add_subplot(221), fig.add_subplot(222), fig.add_subplot(223), fig.add_subplot(224)
    ax1 = fig.add_subplot(pos)

    sub_df = df.loc[df['ticker']==ticker,['call_date']+sentiments]
    sub_df = sub_df.sort_values('call_date',ascending=True)
    print((close_df.index >= sub_df['call_date'].min())&
     (close_df.index <= sub_df['call_date'].max()))
    valid_date_range = \
     ((close_df.index >= sub_df['call_date'].min()) & 
        (close_df.index <= sub_df['call_date'].max() + timedelta(days=days_after)))
    price_sub_df = close_df.loc[valid_date_range,ticker]
    color = 'tab:pink'
    ax1.set_xlabel('date')
    ax1.set_ylabel('sentiment count', color=color)
    ax1.tick_params(axis='y', labelcolor=color)

    ax1.plot(sub_df['call_date'], sub_df[sentiments], marker='o')
    ax1.set_title(ticker)
    plt.setp(ax1.get_xticklabels(), rotation=30, horizontalalignment='right')
    ax1.legend(sentiments, loc='upper left')
    ax1.xaxis.grid() # vertical lines
    color = 'tab:purple'
    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis
    ax2.set_ylabel('share price (close)', color=color)  # we already handled the x-label with ax1
    ax2.plot(price_sub_df.index, price_sub_df, color=color)
    ax2.tick_params(axis='y', labelcolor=color)
fig.tight_layout()  # otherwise the right y-label is slightly clipped
plt.show()